{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a536f5a1-b467-4c08-8b42-a125bacb4249"
   },
   "source": [
    "# DistilBERT for Sentiment Analysis\n",
    "\n",
    "DistilBERT is a **condensed version of BERT** created by Hugging Face:\n",
    "- ðŸš€ **40% fewer parameters**: DistilBERT is a lighter model, offering significant speed and resource advantages.\n",
    "- âš¡ **60% faster inference**: Ideal for real-time applications.\n",
    "- ðŸ“ˆ **95% of BERTâ€™s performance**: Achieves near-parity on benchmarks like **GLUE**, making it highly efficient for natural language understanding tasks.\n",
    "\n",
    "## ðŸ› ï¸ Supported Hardware\n",
    "\n",
    "This notebook can run in a CPU or in a GPU.\n",
    "\n",
    "âœ… AMD Instinctâ„¢ Accelerators  \n",
    "âœ… AMD Radeonâ„¢ RX/PRO Graphics Cards  \n",
    "\n",
    "Suggested hardware: **AMD Instinctâ„¢ Accelerators**, this notebook may not run in a CPU if your system does not have enough memory.\n",
    "\n",
    "## âš¡ Recommended Software Environment\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Linux\n",
    "- [Install Docker container](https://amdresearch.github.io/aup-ai-tutorials//env/env-gpu.html)\n",
    "- [Install PyTorch](https://amdresearch.github.io/aup-ai-tutorials//env/env-cpu.html)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "## ðŸŽ¯ Goals\n",
    "\n",
    "* Fine-tune **DistilBERT**, a lightweight transformer model, to perform **sentiment analysis** on a dataset of movie reviews.\n",
    "* Take advantage of DistilBERTâ€™s efficiency to achieve fast, accurate sentiment classification with fewer parameters.\n",
    "\n",
    "## ðŸ’¡ Problem\n",
    "\n",
    "* The goal is to accurately classify movie reviews into **positive** and **negative** sentiments.\n",
    "* We will:\n",
    "  - Load and preprocess the dataset, splitting it into training, validation, and test sets.\n",
    "  - Use the open-source **`transformers` library** from Hugging Face to tokenize text and load the model.\n",
    "  - Train DistilBERT and evaluate its performance on unseen data, tracking accuracy on the validation and test sets.\n",
    "\n",
    "```{seealso}\n",
    "\n",
    "- **Hugging Face `transformers` Library**  \n",
    "  [Documentation](https://huggingface.co/transformers/) - Explore the open-source library used for NLP model development.\n",
    "\n",
    "- **Understanding BERT and DistilBERT**  \n",
    "  [DistilBERT Research Paper](https://arxiv.org/abs/1910.01108) - Read the original paper for an in-depth understanding of model distillation techniques used to create DistilBERT.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cad3670f-c460-4aea-b0b4-8e3c1ccd2634"
   },
   "source": [
    "## Import Packages\n",
    "\n",
    "Run the following cell to import all the necessary packages to be able to run training and inference using DistilBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ca80ce2a-2e6b-417e-9cf3-1c698f204a21",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the Hugging Face `transformers` library for handling DistilBERT and related NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77ab60b9-1831-443c-997b-f0da7d860174",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ec07e130-4273-4813-81e6-710c8d9a5b05"
   },
   "source": [
    "## Preparing the Dataset\n",
    "\n",
    "We will download the **movie reviews dataset** (compressed in `.gz` format), extract it, and load it into a Pandas DataFrame for further processing.\n",
    "This dataset will be used to fine-tune our DistilBERT model for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://github.com/rasbt/machine-learning-book/raw/main/ch08/movie_data.csv.gz\"\n",
    "filename = os.path.join('datasets', 'movie_data', url.split(\"/\")[-1])\n",
    "\n",
    "if not os.path.isdir(os.path.dirname(filename)):\n",
    "    os.mkdir(os.path.dirname(filename))\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        r = requests.get(url)\n",
    "        f.write(r.content)\n",
    "\n",
    "csv_file = filename.replace('.gz', '.csv')\n",
    "with gzip.open(filename, 'rb') as f_in:\n",
    "    with open(csv_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the extracted CSV file into a Pandas DataFrame and display the first three rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "855304d4-1094-4d72-81c8-167a7656fb5b"
   },
   "source": [
    "## Prepare dataset for Training\n",
    "\n",
    "We will prepare the dataset split into three parts: training, validation, and test sets, selecting the 'review' texts and corresponding 'sentiment' labels for each set.\n",
    "\n",
    "- Training set: First 35,000 reviews and labels\n",
    "- Validation set: Next 5,000 reviews and labels\n",
    "- Test set: Remaining reviews and labels\n",
    "\n",
    "Finally, we printing the sizes of each dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_texts = df.iloc[:35000]['review'].values\n",
    "train_labels = df.iloc[:35000]['sentiment'].values\n",
    "\n",
    "val_texts = df.iloc[35000:40000]['review'].values\n",
    "val_labels = df.iloc[35000:40000]['sentiment'].values\n",
    "\n",
    "test_texts = df.iloc[40000:]['review'].values\n",
    "test_labels = df.iloc[40000:]['sentiment'].values\n",
    "\n",
    "print(f'Training reviews: {len(train_texts):,}, validation reviews: {len(val_texts):,}, test reviews: {len(test_texts):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the device for training\n",
    "\n",
    "Set the CPU or GPU for model training (depending on availability) and ensure reproducibility by fixing random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(123)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ed8e9e70-5fb9-4288-9fa5-c5eed8eb1c14"
   },
   "source": [
    "## Tokenize the Reviews\n",
    "\n",
    "With the splits ready, we will tokenize the review texts using the DistilBERT tokenizer. The idea is to convert the text data into a format that DistilBERT can understand.\n",
    "\n",
    "Each review text is encoded into input IDs and attention masks, `truncation=True` ensures that sequences longer than the model max input length are truncated `padding=True` adds padding to shorter sequences to match the max input length within each batch. We also move the tokenization to the device (CPU or GPU) defined earlier. We do the same for the validation and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16b5a282-b2de-4522-bb35-0087cde0e2e9"
   },
   "source": [
    "## Class to encapsulate the encodings and the labels\n",
    "\n",
    "In this section, we create a PyTorch dataset class that encapsulates the tokenized encodings and their corresponding labels for the IMDb data. This class will be used to create DataLoader objects for training and evaluation.\n",
    "\n",
    "- `__getitem__`: retrieve an item at a specific index\n",
    "- `__len__`: return the length of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f308166a-988d-4def-a844-cb1cefbece12",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class IMDbDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx].clone().detach()) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx]).clone().detach()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de1b16ad-8ed7-4a03-889b-e13d6270d59a"
   },
   "source": [
    "## Create the `Dataloader` objects that will be used in the training loop\n",
    "\n",
    "First, we create instances of the IMDbDataset for training, validation, and test datasets. This wraps the encodings and labels into dataset objects that can be easily used with the DataLoader.\n",
    "\n",
    "Then, we create DataLoader objects for each split. The DataLoader will handle batching, shuffling, and parallel data loading during training and evaluation.\n",
    "- `batch_size=16` means that each batch will contain 16 samples\n",
    "- `shuffle=True` ensures that the data is shuffled every epoch to improve model generalization, this is used for the training set only.\n",
    "\n",
    "Validation and test sets are not shuffled, as we want to evaluate the model on the same data order each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecdfce06-ad94-4f18-abf8-3883abe4f963",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f317256d-2806-4001-8eeb-2a5440238167"
   },
   "source": [
    "## Get a Pretrained BERT Model and Fine-tune It\n",
    "\n",
    "In this section, we will load a pretrained DistilBERT model for sequence classification. We will set up the model for training and specify the optimizer. We will then train the model for a defined number of epochs, logging the loss during the training process.\n",
    "\n",
    "We download the pretrained DistilBERT model from Hugging Face's model hub, specifically the `distilbert-base-uncased` variant, which is a smaller, faster version of BERT, then we move the model to the specified device (CPU or GPU). To drive the training process, we will use the `Adam` optimizer. The learning rate is set to `5e-5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58d96b0a-6d02-4d86-afa1-cc819bed48da"
   },
   "source": [
    "## Model Fine-tuning Loop\n",
    "\n",
    "First, we define the number of epochs to train the model, set to 2 (you can increase this value). We also create an object to track the training loss, which we will use to understand how well the model is learning during training. We also set the model to training mode using `model.train()`, which enables track of back propagation.\n",
    "\n",
    "In the training loop, we iterate over the training DataLoader, which provides batches of data. For each batch, we get the input IDs, attention masks, and labels, and move them to the specified device (CPU or GPU). We then invoke the model with these parameters and extract the loss and logists from the model's output. The loss is then used to perform back propagation and update the model's weights using the optimizer. Finally, we log the training loss for each batch and print a message every 250 batches to monitor progress.\n",
    "\n",
    "To conclude, we compute the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "losses = []\n",
    "model.train()\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss, logits = outputs['loss'], outputs['logits']\n",
    "\n",
    "        # Backward pass\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if batch_idx % 250 == 0:\n",
    "            print(f'Epoch: {epoch+1:02d}/{epochs:02d} | Batch: {batch_idx:04d}/{len(train_loader):04d} | Loss: {loss:.4f}')\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "print(f'It took {train_time/60:.2f} minutes to finetune the BERT model for {epochs} epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training, we can visualize the training loss over epochs to understand how the model learned during training. This will help us identify if the model is converging or if there are any issues like overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, label='Training Loss', color='blue')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now can compute the accuracy of the model on the different splits. To do this, we create a function that gets the model, the data loader and the device.\n",
    "\n",
    "In the function, first we disable gradient tracking (`torch.no_grad()`), this saves computation but also it is not necessary for evaluation. We will keep track of the correctly predicted and the total number of examples. We iterate over the samples on the `data_loader`. For each batch, we get the input IDs, attention masks, and labels, and move them to the specified device (CPU or GPU). We then invoke the model with these parameters and extract the logists from the model's output. With this information we can get the `predicted_labels` and compared them against the actual labels, we count the number of correct ones. Finally, we print the correctly predicted and the total samples. Finally, we return the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    \"\"\"Computes the accuracy of the model on the given data loader.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        data_loader: DataLoader for the dataset (train, validation, or test).\n",
    "        device: The device (CPU or GPU) on which the model and data are loaded.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy as a percentage.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        correct_pred, num_examples = 0, 0\n",
    "        for _, batch in enumerate(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs['logits']  # Get the logits from the model output\n",
    "            predicted_labels = torch.argmax(logits, dim=1)\n",
    "\n",
    "            num_examples += labels.size(0)\n",
    "            correct_pred += (predicted_labels == labels).sum()\n",
    "\n",
    "    print(f'{correct_pred=} {num_examples=}')\n",
    "    return correct_pred.item() / num_examples * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can call this function for the various splits to get the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_accuracy = compute_accuracy(model, train_loader, device)\n",
    "val_accuracy = compute_accuracy(model, val_loader, device)\n",
    "test_accuracy = compute_accuracy(model, test_loader, device)\n",
    "\n",
    "print(f'Training accuracy: {train_accuracy:.2f}%\\nValidation accuracy: {val_accuracy:.2f}%\\nTest accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To showcase the fine-tuned model we will define a function that takes the index of a review as input and returns the sentiment prediction for that review. First, we make sure the index is within bounds. Then, we tokenize the review text and move it to the device. We then perform the sentiment prediction and return the predicted sentiment label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def sentiment_evaluation(index):\n",
    "    if index >= len(test_texts):\n",
    "        index = len(test_texts) - 1\n",
    "\n",
    "    sample_eval = tokenizer(test_texts[index], truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**sample_eval).logits  # Get the logits from the model output\n",
    "\n",
    "    # Return the index of the highest logit value as the predicted sentiment\n",
    "    return logits.argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can test the function with a few random examples. With the predicted sentiments labels we create a DataFrame to display the results, using this dataframe we can visualize the results using `tabulate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for idx in torch.randint(0, 10000, (10,)).tolist():\n",
    "    sent = sentiment_evaluation(idx)\n",
    "    actual_label = test_labels[idx]\n",
    "    results.append({'Index': idx, 'Predicted Sentiment': sent, 'Actual Label': actual_label})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(tabulate(results_df, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can manually enter the index of a review from the table above to get the corresponding sentence. Enter `exit`, to exit the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"Enter the index number from the table above to see the corresponding sentence or type 'exit' to quit: \")\n",
    "\n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"Exiting the program.\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        user_input_index = int(user_input)\n",
    "\n",
    "        if user_input_index in results_df['Index'].values:\n",
    "            predicted_sentiment_new = sentiment_evaluation(user_input_index)\n",
    "            actual_label_new = test_labels[user_input_index]\n",
    "            sentence_new = test_texts[user_input_index]\n",
    "\n",
    "            print(f'Index: {user_input_index}')\n",
    "            print(f'Predicted Sentiment: {predicted_sentiment_new}')\n",
    "            print(f'Actual Label: {actual_label_new}')\n",
    "            print(f'Sentence: {sentence_new}')\n",
    "        else:\n",
    "            print(\"The entered index is not found in the results.\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a valid index number or type 'exit' to quit.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved. Portions of this file consist of AI-generated content.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
