{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f79af8a-936c-4d84-91f5-ad339127a54a",
   "metadata": {},
   "source": [
    "# Fine-tune Llama 3.2 to generate Markdown friendly Python functions\n",
    "\n",
    "In this notebook, we are going to fine tune a Llama 3.2 1B model using QLORA and the [Google Mostly Basic Python Problems](https://huggingface.co/datasets/google-research-datasets/mbpp) dataset.\n",
    "\n",
    "## üõ†Ô∏è Supported Hardware\n",
    "\n",
    "This notebook can run in a CPU or in a GPU.\n",
    "\n",
    "‚úÖ AMD Instinct‚Ñ¢ Accelerators  \n",
    "‚úÖ AMD Radeon‚Ñ¢ RX/PRO Graphics Cards  \n",
    "\n",
    "Suggested hardware: **AMD Instinct‚Ñ¢ Accelerators**, this notebook may not run in a CPU if your system does not have enough memory.\n",
    "\n",
    "## ‚ö° Recommended Software Environment\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Linux\n",
    "- [Install Docker container](https://amdresearch.github.io/aup-ai-tutorials//env/env-gpu.html)\n",
    "- [Install PyTorch](https://amdresearch.github.io/aup-ai-tutorials//env/env-cpu.html)\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "## üéØ Goals\n",
    "\n",
    "- Specialize a model using fine tuning\n",
    "- Quantize the model using BitsandBytes\n",
    "- Define QLoRa parameters\n",
    "- Fine tune using SFTTrainer\n",
    "\n",
    "```{seealso}\n",
    "\n",
    "- This notebook is partially based on the [FluidNumerics](https://www.fluidnumerics.com/) webinar.\n",
    "\n",
    "- [Fine Tuning Llama 3 on AMD Radeon GPUs](https://webinar.amd.com/Fine-Tuning-Llama-3-on-AMD-Radeon-GPUs/en)\n",
    "\n",
    "- [Fine-Tuning Llama-3 on AMD Radeon GPU](https://github.com/FluidNumerics/amd-ml-examples/blob/main/fine-tuning-llama-3/train-single-gpu.ipynb)\n",
    "\n",
    "- [bitsandbytes](https://huggingface.co/docs/bitsandbytes/main/en/index) is a Python wrapper library that offers fast and efficient 8-bit quantization of machine learning models.\n",
    "\n",
    "- [Parameter-Efficient Fine-Tuning](https://huggingface.co/docs/peft/en/index)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ddbfa9",
   "metadata": {},
   "source": [
    "## Get the Model and Tokenizer\n",
    "\n",
    "Import some of the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c6a0aa-fd8a-4fd3-94b2-ca8b34564df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from numpy import argmax\n",
    "\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, LlamaForCausalLM, pipeline, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493bbe7e-c127-40f9-82c4-47c1d01e3514",
   "metadata": {},
   "source": [
    "Select GPU if available, note that a consumer CPU may not be able to fine-tune this model if it does not have enough VRAM memory.\n",
    "\n",
    ":::{note}\n",
    "Using a GPU with large memory is recommended.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b16352-4fc5-4d83-a873-cafb2bb3d56d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Device name: AMD Instinct MI210\n",
      "GPU available memory: 63.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "if device == torch.device(\"cuda\"):\n",
    "    print(f'Device name: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU available memory: {torch.cuda.mem_get_info()[1]/1024/1024//1024} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be29603-0cee-46d3-b52f-020fd3050ac5",
   "metadata": {},
   "source": [
    "Define the model id from HuggingFace, Llama 3.2 1 Billion parameter model. Get the [tokenizer](https://huggingface.co/docs/transformers/v4.46.0/en/model_doc/auto#transformers.AutoTokenizer) and set padding token to the `EOS` token. Also, set `padding_side` to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd13ba5-fe43-491c-8be0-ae305f06a743",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d77c2d30224b718fff8a4a21a8b4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4581130029c548b09a897bc54b163e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1f31277a5549e4981c3ed22f47bb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = 'unsloth/Llama-3.2-1B'\n",
    "\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "my_tokenizer.pad_token = my_tokenizer.eos_token\n",
    "my_tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c9339f-0232-476c-8bed-46d2c742854a",
   "metadata": {},
   "source": [
    "We will use [BitsandBytes](https://github.com/bitsandbytes-foundation/bitsandbytes) to quantize the model. First, we define the `BitsAndBytesConfig`, we will use 4-bit quantization with the `fp4` datatype with nested quantization, finally the computation type is `float16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77edd24f-f122-4ba9-8af4-12977c4d5b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dad8cc77bdab4ac8ae24cd96cd584a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/935 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\n",
      "Copyright (C) 2023 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7920b6d6ce8428dbc99814479174468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688193f6af844327b56e46e7ff69f2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fp4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"fp4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb1bd66",
   "metadata": {},
   "source": [
    "Then we use `transformers.LlamaForCausalLM.from_pretrained` to load the model from Hugging Face and apply the `fp4_config` configuration. We will also set the device that we got before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b01b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = LlamaForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=fp4_config,\n",
    "    device_map=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b772f",
   "metadata": {},
   "source": [
    "## Sample Prompt\n",
    "\n",
    "Now, we will evaluate the model with a sample prompt. We define `transformers.pipeline` for `text-generation` using the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781654a0-11cc-41e0-b464-a04451e34950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:54: UserWarning: Using AOTriton backend for Flash Attention forward... (Triggered internally at /var/lib/jenkins/pytorch/aten/src/ATen/native/transformers/hip/flash_attn/flash_api.h:267.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "write a python function to find duplicate numbers in a list of integer values\n",
      "import  from  collections\n",
      "\n",
      "duplicate_numbers_list = [1,0,3,0,2,3,6]\n",
      "\n",
      "print(dup_number = [i for a if i!= a[i]] for i in enumerate(a.values()) if  i == 1)\n",
      "\n",
      "duplicate_numbers_list = [i  for  i in a.values()\n",
      "                              for  i in enumerate(a)]\n",
      "print(dup number  of numbers = duplicate_numbers)\n",
      "\n",
      "print([i for a if i  for  i in enumerate(a)])\n",
      "```\n",
      "```\n",
      "[1,0,3,0,2,3,0]\n",
      "\n",
      "[0]\n",
      "\n",
      "duplicate_number  of  number s = [i for  i in enumerate(a)]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "sample_prompt = (\n",
    "    r\"write a python function to find duplicate numbers in a list\"\n",
    ")\n",
    "\n",
    "quantized_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=quantized_model,\n",
    "    tokenizer=my_tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d5cdb",
   "metadata": {},
   "source": [
    "Then we can invoke the model to generate an answer to our prompt. We will also print the generated `sequences`.\n",
    "\n",
    ":::{tip}\n",
    "Explore different values of `top_k` and `temperature` and run the prompt twice. What happens if you increase the `temperature`? \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad1f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = quantized_pipeline(\n",
    "    text_inputs=sample_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=my_tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149b038",
   "metadata": {},
   "source": [
    "## Define fine-tune parameters\n",
    "\n",
    "Now, to fine tune the model we will use the Low-Rank Adaption technique. In this technique, instead of modifying the model itself a few extra parameters (rank) are added and then updated during the fine tuning process. For more information, check [here](https://huggingface.co/docs/peft/main/en/developer_guides/lora).\n",
    "\n",
    "We can define the LoRA configuration with `peft.LoraConfig`:\n",
    "- `r`: size of adaptation layer\n",
    "- `lora_alpha`: indicates how strongly does the adaptation layer affect the base model [see 4.1](https://arxiv.org/abs/2106.09685)\n",
    "- `lora_dropout`: optional dropout layer\n",
    "- `bias`: whether or not to set bias\n",
    "- `task_type`: task type see [TaskType](https://huggingface.co/docs/peft/en/package_reference/peft_types#peft.TaskType)\n",
    "- `target_modules`: which modules to apply adapter layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56668ecb-354b-449f-b972-e4c5712f9cd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"k_proj\",\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a94289f",
   "metadata": {},
   "source": [
    "We this configuration, we can define our `adapted_model`, the model we will use the fine tune. And our `adapted_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05598783-21e4-4b7f-8a1f-366933b4967c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "adapted_model = get_peft_model(quantized_model, lora_config)\n",
    "\n",
    "adapted_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=adapted_model,\n",
    "    tokenizer=my_tokenizer,\n",
    "    device_map=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3d0fb",
   "metadata": {},
   "source": [
    "Let's run the `sample_prompt` on the adapted model.\n",
    "\n",
    ":::{tip}\n",
    "Do you note anything different from the original model?\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc5858e-1e23-4e0e-96db-89e11504283e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "write a python function to find duplicate numbers in a list\n",
      "\n",
      "def find_duplicate_numbers(numbers):\n",
      "  \"\"\"Find duplicate numbers in a list.\n",
      "  :param numbers: a list of numbers to search for duplicates.\n",
      "  :returns: a list of numbers that are duplicates.\n",
      "  \"\"\"\n",
      "  duplicates = []\n",
      "  for i in range(len(numbers)):\n",
      "    if numbers[i] == numbers[i+1]:\n",
      "      duplicates.append(numbers[i])\n",
      "  return duplicates\n",
      "\n",
      "\n",
      "Result:\n",
      "write a python function to find duplicate numbers in a list of integers\n",
      "You can use the built-in function to find duplicates in Python. The function is named find_dublicates and it is declared inside the Python standard library.\n",
      "The find_dublicates function takes a list of integers as its argument. It then uses a for loop to iterate over the list and checks if each integer is equal to any of the other integers in the list. If it is, then the function returns a boolean True, which means that there are duplicate numbers in the list. Otherwise, the function returns a boolean False, which means that there are no duplicate numbers in the list.\n",
      "The following code shows how to use the find_dublicates function to find duplicate numbers in a list of integers:\n",
      "list_of_ints = [2, 4, 5, 6, 8, 10, 11, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30]\n",
      "print(\"The list of integers is: \" + str(list_of_ints))\n",
      "print(\"There are no duplicate numbers in the list.\")\n",
      "print(\"The list of integers is: \" + str(list_of_ints))\n",
      "The output of the code is as follows:\n",
      "The list of integers\n",
      "\n",
      "Result:\n",
      "write a python function to find duplicate numbers in a list of integers\n",
      "1. Write a Python function to find duplicate numbers in a list of integers. The function should return a list of tuples. The first element of each tuple should be the number of times the number occurs in the list, and the second element should be the number of times the number occurs in the list. The function should also print a message indicating whether the number occurs more than once in the list. For example, if the input list is [1, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5], the function should return the list [[2, 2, 2], [4, 4, 4], [5, 5, 5]].\n",
      "2. Write a Python function to find duplicate numbers in a list of integers. The function should return a list of tuples. The first element of each tuple should be the number of times the number occurs in the list, and the second element should be the number of times the number occurs in the list. The function should also print a message indicating whether the number occurs more than once in the list. For example, if the input list is [1, 2, \n",
      "\n",
      "Result:\n",
      "write a python function to find duplicate numbers in a list\n",
      "The function returns True if the given list has at least one duplicate, and False otherwise. It returns False if the given list is empty.\n",
      "A function that returns true if the given list has at least one duplicate, and false if the given list is empty.\n",
      "A function that returns true if the given list has at least one duplicate, and false if the given list is empty. It returns false if the given list is empty.\n",
      "A function that returns true if the given list has at least one duplicate, and false if the given list is empty. It returns False if the given list is empty.\n",
      "A function that returns true if the given list has at least one duplicate, and false if the given list is empty. It returns False if the given list is empty.\n",
      "A function that returns true if the given list has at least one duplicate, and false if the given list is empty. It returns False if the given list is empty.\n",
      "A function that returns true if the given list has at least one duplicate, and false if the given list is empty. It returns False if the given list is empty.\n",
      "A function that returns true if the given list has at least one duplicate, and false if the given list is empty. It returns False if the given list is empty\n",
      "\n",
      "Result:\n",
      "write a python function to find duplicate numbers in a list\n",
      "I have a list of numbers that are duplicates, how can I find the duplicate numbers in the list?\n",
      "The problem is that I need to find the duplicates numbers in the list. I have tried the code below but it is not working.\n",
      "list = [1,2,3,4,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,\n"
     ]
    }
   ],
   "source": [
    "sequences = adapted_pipeline(\n",
    "    text_inputs=sample_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=my_tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117114e",
   "metadata": {},
   "source": [
    "## Get Dataset to fine-tune model\n",
    "\n",
    "We are going to use the [Google Mostly Basic Python Problems](https://huggingface.co/datasets/google-research-datasets/mbpp) dataset. Although, large language models are very good at Python, the idea of this example is to fine-tune the model into providing the output in a particular style. It may be possible to get similar results with prompt-engineering techniques, however the idea of the notebook is to show you an example of fine-tuning.\n",
    "\n",
    "Load dataset and print it.\n",
    "\n",
    "\n",
    ":::{note}\n",
    "By executing the next cell, you will download the dataset `google-research-datasets/mbpp` and you agree to its license and obtaining permission to use it from dataset owner if needed.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9187eb44-1865-4235-af4b-ecea1f843903",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['source_file', 'task_id', 'prompt', 'code', 'test_imports', 'test_list'],\n",
      "        num_rows: 120\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['source_file', 'task_id', 'prompt', 'code', 'test_imports', 'test_list'],\n",
      "        num_rows: 257\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['source_file', 'task_id', 'prompt', 'code', 'test_imports', 'test_list'],\n",
      "        num_rows: 43\n",
      "    })\n",
      "    prompt: Dataset({\n",
      "        features: ['source_file', 'task_id', 'prompt', 'code', 'test_imports', 'test_list'],\n",
      "        num_rows: 7\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "google_python = load_dataset(\"google-research-datasets/mbpp\", \"sanitized\")\n",
    "\n",
    "print(google_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9d6bf0",
   "metadata": {},
   "source": [
    "We are now going to define the output format that we want the model to be fine tuning on using [chat templates](https://huggingface.co/blog/chat-templates). The task is to fine-tune the model so the output Python is Markdown friendly, i.e., being able to print code snippets.\n",
    "\n",
    "The function `instructify` receives the `qr_row` dictionary that contains the `prompt`, `code` and `test_list`. We define the `qr_json` template with the `user` and `assistant` role. The user role contains the `prompt` and the `assistant` role contains the Python `code` as snippet and the test list. Finally, we apply the `apply_chat_template` to the roles dict and add it to the `text` key and return `qr_row`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b3b775-98c0-450c-b94a-c9b4c3cf7b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def instructify(qr_row):\n",
    "    qr_json = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": qr_row[\"prompt\"],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f'''\n",
    "```python\n",
    "{qr_row[\"code\"]}\n",
    "```\n",
    "\n",
    "Test List:\n",
    "\n",
    "```python\n",
    "test_list={qr_row[\"test_list\"]}\n",
    "```\n",
    "''',\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    qr_row[\"text\"] = my_tokenizer.apply_chat_template(qr_json, tokenize=False)\n",
    "    return qr_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a3b0a",
   "metadata": {},
   "source": [
    "We will define the chat template. Check Llama-3 prompt formats [here](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/). Concatenating query/response is sufficient for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64321297-e71a-4462-9b11-8187326e808b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = message['content'] | trim + '\n",
      "' %}{{ content }}{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer.chat_template = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = message['content'] | trim + '\\n' %}{{ content }}{% endfor %}\"\"\"\n",
    "\n",
    "print(my_tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681897a",
   "metadata": {},
   "source": [
    "We now can apply the chat template to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454f180-0251-4961-a433-729916fd18d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f2d45bce174e24bb75f09ee7f20e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c64953e334420c8203e97fadb0cd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76e3a80589b4e0682e9b32edad918c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/43 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3bb906841a44fd851da1c42cdef0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "formatted_dataset = google_python.map(instructify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f9290f",
   "metadata": {},
   "source": [
    "Display one example, you can see how the dataset now is formatted to show code snippets (```). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6dee0bf-6cea-498b-a2f1-860894906323",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a python function to find the first repeated character in a given string.\n",
      "```python\n",
      "def first_repeated_char(str1):\n",
      "  for index,c in enumerate(str1):\n",
      "    if str1[:index+1].count(c) > 1:\n",
      "      return c\n",
      "```\n",
      "\n",
      "Test List:\n",
      "\n",
      "```python\n",
      "test_list=['assert first_repeated_char(\"abcabc\") == \"a\"', 'assert first_repeated_char(\"abc\") == None', 'assert first_repeated_char(\"123123\") == \"1\"']\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(formatted_dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5972ad",
   "metadata": {},
   "source": [
    "Display the same content using the `IPython.display.Markdown` visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b20b9-bc21-481f-b098-0ac95c16c980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Write a python function to find the first repeated character in a given string.\n",
       "```python\n",
       "def first_repeated_char(str1):\n",
       "  for index,c in enumerate(str1):\n",
       "    if str1[:index+1].count(c) > 1:\n",
       "      return c\n",
       "```\n",
       "\n",
       "Test List:\n",
       "\n",
       "```python\n",
       "test_list=['assert first_repeated_char(\"abcabc\") == \"a\"', 'assert first_repeated_char(\"abc\") == None', 'assert first_repeated_char(\"123123\") == \"1\"']\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "Markdown(formatted_dataset[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3491458",
   "metadata": {},
   "source": [
    "Let's run this example prompt on the adapted model and observe the output. Although, we see some code snippet, the test list is not there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc388f3d-5e32-48f4-a289-90ba31e5db47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "Write a python function to remove first and last occurrence of a given character from the string. \n",
      "The function should return a new string with the given character removed from the string.\n",
      "\n",
      "For example, if the string is 'hello', the function should return 'hello' and if the string is 'world', the function should return 'w'.\n",
      "\n",
      "Hint: use the `count` method to count the number of occurrences of a character in the string and then remove the first and last occurrences of the character.\n",
      "```python\n",
      "string = 'hello'\n",
      "char = 'o'\n",
      "result = string.count(char)\n",
      "print(f\"String '{string}' has character '{char}' {result} times\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "example_prompt = formatted_dataset[\"test\"][0][\"prompt\"]\n",
    "\n",
    "sequences = adapted_pipeline(\n",
    "    text_inputs=example_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=my_tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"\\nResult:\\n{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d88909",
   "metadata": {},
   "source": [
    "## üöÄ Fine-tune the Adapted Model\n",
    "\n",
    "We now define the metric that will be used to [evaluate](https://huggingface.co/docs/evaluate/package_reference/loading_methods#evaluate.load) the fine-tuned model, we will use [accuracy](https://huggingface.co/docs/evaluate/v0.4.0/en/types_of_evaluations#metrics). We will also define the loss function with the `compute_metric` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f41e36f-861d-42d3-8e26-918747b64a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = argmax(logits, axis=-1)\n",
    "    return evaluate.metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6590c22",
   "metadata": {},
   "source": [
    "We also need to tokenize the dataset before it can be consumed in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9594ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset, tokenizer, text_field):\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[text_field], truncation=True, padding=True)\n",
    "\n",
    "    return dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "tokenized_train_dataset = tokenize_dataset(formatted_dataset[\"train\"], my_tokenizer, \"text\")\n",
    "tokenized_eval_dataset = tokenize_dataset(formatted_dataset[\"test\"], my_tokenizer, \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6cf7c",
   "metadata": {},
   "source": [
    "Let's define our training configuration, we do this with `trl.SFTConfig`, some of the most relevant arguments are listed below:\n",
    "- `per_device_train_batch_size`: size of the training batch\n",
    "- `per_device_eval_batch_size`: size of the evaluation batch\n",
    "- `gradient_accumulation_steps`: Gradient accumulation steps\n",
    "- `optim`: optimizer type\n",
    "- `num_train_epochs`: number of training epochs\n",
    "- `eval_steps`: evaluation steps\n",
    "- `logging_steps`: how often the model logs progress\n",
    "- `warmup_steps`: warmup steps\n",
    "- `learning_rate`: rate of learning\n",
    "- We use `fp16` precision\n",
    "- `group_by_length`: Group samples by length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17910750-cf1a-443a-baa7-1f4fa6f5a76f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62db1140fd4840ecac6e25dee31ffd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a13f761e304ba087c9b23a9f0981d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c820b6ea18e84552ad7af7a6f485a38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88f8fc364be409e95e60db4efb3c60d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "sft_config = SFTConfig(\n",
    "    output_dir=\"Llama-Python-Single-GPU\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=20,\n",
    "    eval_steps=0.5,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    max_seq_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014fa317",
   "metadata": {},
   "source": [
    "With the configuration defined, we can finally create the `trl.SFTTrainer` that will help us with the fine tuning.\n",
    "We initialize it with the `adapted_model`, the tokenized tran and eval datasets, the `SFTConfig` and the `lora_config`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=adapted_model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    args=sft_config,\n",
    "    peft_config=lora_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c666e1",
   "metadata": {},
   "source": [
    "Finally, we can call the `.train()` method to start fine tuning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f23a31-de04-435a-9aaf-87d8a3791844",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/autograd/graph.py:823: UserWarning: Using AOTriton backend for Flash Attention backward... (Triggered internally at /var/lib/jenkins/pytorch/aten/src/ATen/native/transformers/hip/flash_attn/flash_api.h:452.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 03:10, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.569500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.826900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.186300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.832100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.877700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.662100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.608500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.515600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.524200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.563000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.554100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.597600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.570800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.508100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.622500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.454400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.446500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.461600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.439300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.437300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.505800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.493600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.464200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.338600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.363300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.409900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.428500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.409100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.439500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.455900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.349300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.372000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.376100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.296200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.478900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.296300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.437200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.266100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.326300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.349800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.369600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.334000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.330900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.334200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.306300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.337900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.298800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.292300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.311600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.276300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.293100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.254400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.315400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.283700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.294100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.232200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.269200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.263800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.248200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.271100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.248100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.251400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.260100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.199800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.209500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.199900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.159800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.143900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.128100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.204300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.199300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.115300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.127300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.178400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.179700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.141700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.126400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.134100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.108700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.117300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.134700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.098500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.090500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.105100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.077800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.064200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.080300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.078100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.088600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.066500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.156200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.082900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.053700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.087200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.091500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.056800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.073000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.084000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.076300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.041400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.065400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.042200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.065700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.056600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.082900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.053800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.058700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.037500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.039600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.071200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.047700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.055200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.040100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.039600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.045400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.029900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.064100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.026200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.038700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.024300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.023000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.025700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.044700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.040800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.039600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.2689305164354543, metrics={'train_runtime': 192.0013, 'train_samples_per_second': 12.5, 'train_steps_per_second': 1.562, 'total_flos': 6322328115609600.0, 'train_loss': 0.2689305164354543})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee858e",
   "metadata": {},
   "source": [
    "You can decide to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3f05f-61cb-4be8-b1a1-0541c65f6145",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = False\n",
    "if save_model:\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2029d2",
   "metadata": {},
   "source": [
    "## Evaluate Fine-tuned Model\n",
    "\n",
    "After the fine tuning, we can evaluate if we achieved our desired outcome. Let us define a different prompt and invoke the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0347e103-de9b-4c1c-a757-7b26357a1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = r\"write a python function that returns the least common denominator of all elements in a list.\"\n",
    "\n",
    "sequences = adapted_pipeline(\n",
    "    text_inputs=example_prompt,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=my_tokenizer.eos_token_id,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f391408",
   "metadata": {},
   "source": [
    "Display the generated text using the Markdown display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49abf880-7b97-40c8-8616-8f04a8cb78b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "write a python function that returns the least common denominator of all elements in a list. https://www.geeksforgeeks.org/least-common-denominator/\n",
       "```python\n",
       "def lcm_of_elements(arr):\n",
       "    (left, right) = (arr[0], arr[-1])\n",
       "    for m in (le, rt):\n",
       "        if (m == left or m == m * right / m):\n",
       "            return m\n",
       "        else:\n",
       "            return m\n",
       "```\n",
       "\n",
       "Test List:\n",
       "\n",
       "```python\n",
       "test_list=['assert lcm_of_elements([2,2,1])->1', 'assert lcm_of_elements([1,5,7,1])->5', 'assert lcm_of_elements([12,45,67,12])->45']\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(sequences[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46221ef",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you quantized a Llama model, then added LoRA to adapt the model to be able to train on a custom dataset. You also defined chat templates that guided the fine-tuning process.\n",
    "\n",
    "Now, you may be wondering how much bigger is the adapted model. Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c4a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_quant = summary(quantized_model, input_size=(1, 112, 112), col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\", \"trainable\"])\n",
    "model_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapt_model_quant = summary(adapted_model, input_size=(1, 112, 112), col_names=[\"input_size\", \"output_size\", \"num_params\", \"mult_adds\", \"trainable\"])\n",
    "adapt_model_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a3927",
   "metadata": {},
   "source": [
    "----------\n",
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved. Portions of this file consist of AI-generated content.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
