{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b784bcff-d3b8-4d1e-a6ef-c1ce7e239fd4",
   "metadata": {},
   "source": [
    "# Fine-tuned LAnguage Net Text-To-Text Transfer Transformer\n",
    "\n",
    "Fine-tuned LAnguage Net Text-To-Text Transfer Transformer (in short FLAN-T5) is a sequence-to-sequence, large language model published by Google in late 2022. It has been pre-trained on prompting datasets. This means that the model has knowledge of performing specific tasks such as summarization, classification and translation, etc. The model architecture is very similar to the encoder-decoder transformer architecture defined in the revolutionary [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper.\n",
    "\n",
    "## üõ†Ô∏è Supported Hardware\n",
    "\n",
    "This notebook can run in a CPU or in a GPU.\n",
    "\n",
    "‚úÖ AMD Instinct‚Ñ¢ Accelerators  \n",
    "‚úÖ AMD Radeon‚Ñ¢ RX/PRO Graphics Cards  \n",
    "‚úÖ AMD EPYC‚Ñ¢ Processors  \n",
    "‚úÖ AMD Ryzen‚Ñ¢ (AI) Processors  \n",
    "\n",
    "## ‚ö° Recommended Software Environment\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Linux\n",
    "- [Install Docker container](https://amdresearch.github.io/aup-ai-tutorials//env/env-gpu.html)\n",
    "- [Install PyTorch](https://amdresearch.github.io/aup-ai-tutorials//env/env-cpu.html)\n",
    ":::\n",
    "\n",
    ":::{tab-item} Windows\n",
    "- [Install Direct-ML](https://amdresearch.github.io/aup-ai-tutorials//env/env-gpu-windows.html)\n",
    "- [Install PyTorch](https://amdresearch.github.io/aup-ai-tutorials//env/env-cpu.html)\n",
    ":::\n",
    "::::\n",
    "\n",
    "## üéØ Goals\n",
    "\n",
    "- Show you how to download a model from HuggingFace\n",
    "- Run FLAN-T5 on an AMD platform\n",
    "- Execute a few prompts\n",
    "\n",
    ":::{seealso}\n",
    "- [FLAN-T5](https://huggingface.co/docs/transformers/en/model_doc/flan-t5)\n",
    "- [Hugging Face Flan T5 small](https://huggingface.co/google/flan-t5-small)\n",
    "- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d73b2a",
   "metadata": {},
   "source": [
    "## üöÄ Run FLAN-T5 on an AMD Platform\n",
    "\n",
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076e6ad-8a30-4b12-9241-e1acf8dd6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f647ba4d-fc29-4722-bfc7-0c4ed2766722",
   "metadata": {},
   "source": [
    "Check if GPU can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8a0e3-5e04-4001-a2f0-7dbc277e920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9b298",
   "metadata": {},
   "source": [
    "Download both the tokenizer and the model from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d3c56-d58e-4930-9704-49b76b4ef138",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_id, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_id, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97991237",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model size: {model.num_parameters() * model.dtype.itemsize / 1024 / 1024:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4274d7b-59b5-4f62-a8a7-3884c7807f6e",
   "metadata": {},
   "source": [
    "Let us translate to German, first we provide our prompt to the tokenizer that returns the tokens in tensor format, also the tokens are moved to the device (if a GPU is available). Then, we pass the tokens to the model and define the maximum number of tokens in the inference. Finally, we use the tokenizer decoder to print the German text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e72669b-2401-4ffe-98eb-8a7f587fe05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = model.generate(input_ids, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893be40",
   "metadata": {},
   "source": [
    "Let do sentiment analysis, we will use on entry of the [Poem Dataset](https://huggingface.co/datasets/google-research-datasets/poem_sentiment?row=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7949ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Review: when i peruse the conquered fame of heroes, and the victories of mighty generals, i do not envy the generals. Sentiment:\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = model.generate(input_ids, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de40de",
   "metadata": {},
   "source": [
    "Let's use for summary. We're using on of the entries of [Wikipedia Summary Dataset](https://huggingface.co/datasets/jordiclive/wikipedia-summary-dataset?row=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cfcdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\" summarize: \n",
    "Amy: Hey Mark, have you heard about the new movie coming out this weekend?\n",
    "Mark: Oh, no, I haven't. What's it called?\n",
    "Amy: It's called \"Stellar Odyssey.\" It's a sci-fi thriller with amazing special effects.\n",
    "Mark: Sounds interesting. Who's in it?\n",
    "Amy: The main lead is Emily Stone, and she's fantastic in the trailer. The plot revolves around a journey to a distant galaxy.\n",
    "Mark: Nice! I'm definitely up for a good sci-fi flick. Want to catch it together on Saturday?\n",
    "Amy: Sure, that sounds great! Let's meet at the theater around 7 pm.\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = model.generate(input_ids, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a096d8-4b91-46d9-862e-c46cfebc2d3d",
   "metadata": {},
   "source": [
    "We can also ask about AMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2960866d-42c4-4b08-bfde-925d2bf67be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What does AMD do?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47745a3c-3012-42a1-a3d9-e04e168a34fe",
   "metadata": {},
   "source": [
    "You can check the raw tokens for both the input to the model as well as the inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d636c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93137248-d2c7-4fac-b005-4cc7c1b43a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49560ea4-34b8-4ada-84f5-91ac7ddcad14",
   "metadata": {},
   "source": [
    "We can also ask about math problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de348de1-0d6d-4cb5-acc5-20160f265872",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"The square root of x is the cube root of y. What is y to the power of 2, if x = 4?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "outputs = model.generate(input_ids, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee24c504",
   "metadata": {},
   "source": [
    "----------\n",
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
