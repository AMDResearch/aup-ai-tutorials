{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521fde4a-f7e4-4a5b-a560-71ce5bc8622b",
   "metadata": {},
   "source": [
    "# Phi-3 Instruct Open Model\n",
    "\n",
    "The Phi-3-Mini-4K-Instruct is a 3.8B parameters model with 4K context length. The [model](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct#model) is a dense decoder-only Transformer model which is fine-tuned with Supervised fine-tuning (SFT) and Direct Preference Optimization (DPO) to ensure alignment with human preferences and safety guidelines. This model supports a vocabulary size of up to 32,064 tokens.\n",
    "\n",
    "The model has been designed for general purpose AI systems and applications which require:\n",
    "\n",
    "- memory/compute constrained environments\n",
    "- latency bound scenarios and\n",
    "- strong reasoning (especially math and logic)\n",
    "\n",
    "## üõ†Ô∏è Supported Hardware\n",
    "\n",
    "This notebook can run in a CPU or in a GPU.\n",
    "\n",
    "‚úÖ AMD Instinct‚Ñ¢ Accelerators  \n",
    "‚úÖ AMD Radeon‚Ñ¢ RX/PRO Graphics Cards  \n",
    "‚ö†Ô∏è AMD EPYC‚Ñ¢ Processors  \n",
    "‚ö†Ô∏è AMD Ryzen‚Ñ¢ (AI) Processors  \n",
    "\n",
    "Suggested hardware: **AMD Instinct‚Ñ¢ Accelerators**, this notebook can run in a CPU as well but inference is CPU will be slow.\n",
    "\n",
    "## ‚ö° Recommended Software Environment\n",
    "\n",
    "::::{tab-set}\n",
    "\n",
    ":::{tab-item} Linux\n",
    "- [Install Docker container](https://amdresearch.github.io/aup-ai-tutorials//env/env-gpu.html)\n",
    "- [Install PyTorch](https://amdresearch.github.io/aup-ai-tutorials//env/env-cpu.html)\n",
    ":::\n",
    "\n",
    ":::{tab-item} Windows\n",
    "- [Install Direct-ML](https://amdresearch.github.io/aup-ai-tutorials//env/env-gpu-windows.html)\n",
    "- [Install PyTorch](https://amdresearch.github.io/aup-ai-tutorials//env/env-cpu.html)\n",
    ":::\n",
    "::::\n",
    "\n",
    "## üéØ Goals\n",
    "\n",
    "- Show you how to download a model from HuggingFace\n",
    "- Run Phi-3 Instruct on an AMD platform\n",
    "- Prompt the model and explore system and user role prompts\n",
    "\n",
    "\n",
    ":::{seealso}\n",
    "- [Phi-3-Mini-4K-Instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n",
    "- [Phi open models](https://azure.microsoft.com/en-us/products/phi/)\n",
    "- [Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone](https://arxiv.org/abs/2404.14219)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344a5b52",
   "metadata": {},
   "source": [
    "## üöÄ Run Phi-3 Instruct on an AMD Platform\n",
    "\n",
    "Import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42502ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4097c1b5",
   "metadata": {},
   "source": [
    "Check if GPU is available for acceleration.\n",
    "\n",
    "```{note}\n",
    "Running the model on a GPU is strongly recommended. If your device is `cpu`, the model token generation will be slow.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c3c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'{device=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951fc702",
   "metadata": {},
   "source": [
    "Download model and tokenizer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b1c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbe4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model size: {model.num_parameters() * model.dtype.itemsize / 1024 / 1024:.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c92fbf",
   "metadata": {},
   "source": [
    "Define pipeline and generation arguments.\n",
    "We are going to use the [transformers pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) API to create the model call and pass the user prompt.\n",
    "\n",
    "We start by creating a pipeline object with the goal of `text-generation`, we also specify the model and the tokenizer.\n",
    "\n",
    "The `generation_args` is a helper dictionary that we will pass to the pipeline object, we specify certain parameters, such as the max tokens, temperature (creativity of the model) and sample (if True it would select from the most likely output tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58055584",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.01,\n",
    "    \"do_sample\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83acfab6",
   "metadata": {},
   "source": [
    "Let's define a system prompt for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06424f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56babf34",
   "metadata": {},
   "source": [
    "Define a simple prompt asking about a simple math problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    system_prompt,\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93491260",
   "metadata": {},
   "source": [
    "Generate model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d58cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(prompt, **generation_args) \n",
    "print(f'Prompt:\\n {prompt[1][\"content\"]}\\n\\nResponse:\\n{output[0][\"generated_text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d7be9",
   "metadata": {},
   "source": [
    "The response is good, but we want the model to respond in a more concise way, for this we are going to use  [few-shot prompting](https://www.promptingguide.ai/techniques/fewshot), in this case we will do one-shot prompting. In the prompt fed to the model, we are providing an example of how we would like the response to look like.\n",
    "\n",
    "- In the system prompt we define that the model is a helpful assistant\n",
    "- Then we provide the user question and and example of how we would like the model to answer, the we finally include the actual question we would like the model to reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8dab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_oneshot = [\n",
    "    system_prompt,\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de74f3",
   "metadata": {},
   "source": [
    "Generate model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c9e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipe(messages_oneshot, **generation_args) \n",
    "print(f'Prompt:\\n {messages_oneshot[3][\"content\"]}\\n\\nResponse:\\n{output[0][\"generated_text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230383d3",
   "metadata": {},
   "source": [
    "Note how the response from the model is more concise now.\n",
    "\n",
    "```{tip}\n",
    "Exercise for the readers, modify the `generation_args` configuration, for instance increase the value of `temperature` (max is `2.0`) and set `do_sample` to `True`. What is the outcome?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf1100",
   "metadata": {},
   "source": [
    "----------\n",
    "Copyright (C) 2025 Advanced Micro Devices, Inc. All rights reserved.\n",
    "\n",
    "SPDX-License-Identifier: MIT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
